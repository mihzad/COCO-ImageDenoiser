{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9EsqSsCJukO",
        "outputId": "53a9905e-b26f-4ca2-89a2-1be01a067ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W4PLneNsNDj"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.unpack_archive(\"/content/drive/MyDrive/COCO2017/cocomy.zip\", \"/content/cocomy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SJR50gA8kCx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "Weh7T1BvGEC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1237Yiw1Umrn",
        "outputId": "548de11d-88b6-49b3-f7da-b14d54f996f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "print(pandas.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bl7Wy4NzkRv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.transforms import v2, functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRnms4PoCCLC"
      },
      "outputs": [],
      "source": [
        "class CocoDenoisingDataset(Dataset):\n",
        "    def __init__(self, root_dir, input_img_transform, target_img_transform, noise_factor=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Шлях до папки з картинками.\n",
        "            img_size (tuple): Розмір, до якого треба ресайзити (h, w).\n",
        "            noise_factor (float): Сила шуму (від 0.0 до 1.0).\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.input_transform = input_img_transform\n",
        "        self.target_transform = target_img_transform\n",
        "\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        input_img = self.input_transform(image)\n",
        "        target_img = self.target_transform(image)\n",
        "\n",
        "        return input_img, target_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmCI4QrCR17i"
      },
      "outputs": [],
      "source": [
        "class LayerNorm2d(nn.Module):\n",
        "    \"\"\"\n",
        "    LayerNorm, for (N, C, H, W) shape.\n",
        "    torch`s default LayerNorm expects (N, H, W, C) which needs costful permutes.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(num_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(num_features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.eps)\n",
        "\n",
        "        # 4D-equivalent of (x * weight + bias)\n",
        "        return self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "\n",
        "class SimpleGate(nn.Module):\n",
        "    \"\"\"\n",
        "    NAFNet feature. Replacer of ReLU/GELU.\n",
        "    Divides C dim in half and performs C1*C2.\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        x1, x2 = x.chunk(2, dim=1)\n",
        "        return x1 * x2\n",
        "\n",
        "class NAFBlock(nn.Module):\n",
        "    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0.):\n",
        "        super().__init__()\n",
        "        dw_channel = c * DW_Expand\n",
        "        self.conv1 = nn.Conv2d(c, dw_channel, 1)\n",
        "        self.conv2 = nn.Conv2d(dw_channel, dw_channel, 3, padding=1, groups=dw_channel) # Depthwise\n",
        "\n",
        "        self.conv3 = nn.Conv2d(dw_channel // 2, c, 1)\n",
        "\n",
        "        # Simplified Channel Attention (SCA)\n",
        "        self.sca = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(dw_channel // 2, dw_channel // 2, 1)\n",
        "        )\n",
        "\n",
        "        # SimpleGate\n",
        "        self.sg = SimpleGate()\n",
        "\n",
        "        # Feed Forward Network (FFN) part\n",
        "        ffn_channel = FFN_Expand * c\n",
        "        self.conv4 = nn.Conv2d(c, ffn_channel, 1)\n",
        "        self.conv5 = nn.Conv2d(ffn_channel // 2, c, 1)\n",
        "\n",
        "        self.norm1 = LayerNorm2d(c)\n",
        "        self.norm2 = LayerNorm2d(c)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
        "        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
        "\n",
        "        # Layer Scale (параметри, що навчаються, для стабільності)\n",
        "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
        "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = inp\n",
        "\n",
        "        # Частина 1: Spatial Mixing\n",
        "        x = self.norm1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.sg(x)\n",
        "        x = x * self.sca(x) # Множення на увагу\n",
        "        x = self.conv3(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        y = inp + x * self.beta # Residual Connection 1\n",
        "\n",
        "        # Частина 2: Channel Mixing (FFN)\n",
        "        x = self.norm2(y)\n",
        "        x = self.conv4(x)\n",
        "        x = self.sg(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return y + x * self.gamma # Residual Connection 2\n",
        "\n",
        "class NAFNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Gather all the NAFblocks into U-Net structure.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_channel=3, width=32, middle_blk_num=1, enc_blk_nums=[1, 1, 1], dec_blk_nums=[1, 1, 1]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.intro = nn.Conv2d(img_channel, width, 3, padding=1)\n",
        "        self.ending = nn.Conv2d(width, img_channel, 3, padding=1)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        chan = width\n",
        "\n",
        "        for num in enc_blk_nums:\n",
        "            self.encoders.append(nn.Sequential(*[NAFBlock(chan) for _ in range(num)]))\n",
        "            self.downs.append(nn.Conv2d(chan, 2*chan, 2, 2)) # Downsampling (stride 2)\n",
        "            chan = chan * 2\n",
        "\n",
        "        # Middle\n",
        "        self.middle_blks = nn.Sequential(*[NAFBlock(chan) for _ in range(middle_blk_num)])\n",
        "\n",
        "        # Decoder\n",
        "        self.decoders = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "\n",
        "        for num in dec_blk_nums:\n",
        "            self.ups.append(nn.Sequential(\n",
        "                nn.Conv2d(chan, chan * 2, 1, bias=False),\n",
        "                nn.PixelShuffle(2) # Upsampling\n",
        "            ))\n",
        "            chan = chan // 2\n",
        "            self.decoders.append(nn.Sequential(*[NAFBlock(chan) for _ in range(num)]))\n",
        "\n",
        "    def forward(self, inp):\n",
        "\n",
        "        # Basic extraction\n",
        "        x = self.intro(inp)\n",
        "\n",
        "        # Encoder\n",
        "        encs = []\n",
        "        for encoder, down in zip(self.encoders, self.downs):\n",
        "            x = encoder(x)\n",
        "            encs.append(x)\n",
        "            x = down(x)\n",
        "\n",
        "        # Middle\n",
        "        x = self.middle_blks(x)\n",
        "\n",
        "        # Decoder\n",
        "        for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):\n",
        "            x = up(x)\n",
        "            x = x + enc_skip\n",
        "            x = decoder(x)\n",
        "\n",
        "        # Ending\n",
        "        x = self.ending(x)\n",
        "\n",
        "        return x + inp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-msssim\n",
        "from pytorch_msssim import ms_ssim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBYvyS00xMb-",
        "outputId": "c2113761-9971-45a1-94f9-e88d7d94f026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-msssim\n",
            "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from pytorch-msssim) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->pytorch-msssim) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->pytorch-msssim) (3.0.3)\n",
            "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: pytorch-msssim\n",
            "Successfully installed pytorch-msssim-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchSaltAndPepper(v2.Transform):\n",
        "    \"\"\"\n",
        "    Applies Salt and Pepper noise directly to a batch of PyTorch tensors (B, C, H, W).\n",
        "    \"\"\"\n",
        "    def __init__(self, salt_prob: float = 0.01, pepper_prob: float = 0.01):\n",
        "        super().__init__()\n",
        "        self.salt_prob = salt_prob\n",
        "        self.pepper_prob = pepper_prob\n",
        "        self.total_prob = salt_prob + pepper_prob\n",
        "\n",
        "        if not (0.0 <= self.total_prob <= 1.0):\n",
        "            raise ValueError(\"salt_prob + pepper_prob must be between 0.0 and 1.0\")\n",
        "\n",
        "    def _apply_batch(self, batch: torch.Tensor) -> torch.Tensor:\n",
        "        B, C, H, W = batch.shape\n",
        "\n",
        "        rand_tensor = torch.rand(\n",
        "            B, 1, H, W,\n",
        "            device=batch.device,\n",
        "            dtype=batch.dtype\n",
        "        )\n",
        "\n",
        "        salt_mask = rand_tensor < self.salt_prob\n",
        "\n",
        "        pepper_mask = (rand_tensor >= self.salt_prob) & (rand_tensor < self.total_prob)\n",
        "\n",
        "\n",
        "        batch[salt_mask.expand_as(batch)] = 1.0  # Apply Salt\n",
        "        batch[pepper_mask.expand_as(batch)] = 0.0 # Apply Pepper\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def __call__(self, inpt):\n",
        "        # single image is also \"supported\" - treated as a batch for _apply_batch()\n",
        "        if isinstance(inpt, torch.Tensor):\n",
        "            if inpt.dim() == 4:\n",
        "                return self._apply_batch(inpt)\n",
        "            elif inpt.dim() == 3:\n",
        "                return self._apply_batch(inpt.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "        # default v2.Transform behavior for other types\n",
        "        return super().__call__(inpt)"
      ],
      "metadata": {
        "id": "h94ajK7P42NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvmCaBLEDRix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff913ff-3c5f-4188-d5a0-548edc263fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len_train: 25000; len_val: 5000; len_test: 40670.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "img_size = 256\n",
        "rand_state = 44\n",
        "num_workers = 2\n",
        "\n",
        "def relative_data_path(path: str):\n",
        "  return \"/content/cocomy/data/\"+ path\n",
        "\n",
        "def relative_drive_path(path: str):\n",
        "  return \"/content/drive/MyDrive/COCO2017/\" + path\n",
        "\n",
        "def curr_time():\n",
        "    return datetime.now(ZoneInfo('Europe/Kiev'))\n",
        "\n",
        "\n",
        "def printshare(msg, logfile=relative_drive_path(\"training_log.txt\")):\n",
        "    print(msg)\n",
        "\n",
        "    with open(logfile, \"a\") as f:\n",
        "        print(msg, file=f)\n",
        "\n",
        "\n",
        "def cosannealing_decay_warmup(warmup_steps, T_0, T_mult, decay_factor, base_lr, eta_min):\n",
        "    # returns the func that performs all the calculations.\n",
        "    # useful for keeping all the params in one place = scheduler def.\n",
        "    def lr_lambda(epoch): #0-based epoch\n",
        "        if epoch < warmup_steps:\n",
        "            return base_lr * ((epoch + 1) / warmup_steps)\n",
        "\n",
        "        annealing_step = epoch - warmup_steps\n",
        "\n",
        "        # calculating which cycle (zero-based) are we in,\n",
        "        # current cycle length (T_current) and position inside the cycle (t)\n",
        "        if T_mult == 1:\n",
        "            cycle = annealing_step // T_0\n",
        "            t = annealing_step % T_0\n",
        "            T_current = T_0\n",
        "\n",
        "        else:\n",
        "            # fast log-based computation\n",
        "            cycle = int(math.log((annealing_step * (T_mult - 1)) / T_0 + 1, T_mult))\n",
        "            sum_steps_of_previous_cycles = T_0 * (T_mult ** cycle - 1) // (T_mult - 1)\n",
        "            t = annealing_step - sum_steps_of_previous_cycles\n",
        "            T_current = T_0 * (T_mult ** cycle)\n",
        "\n",
        "\n",
        "        # enable decay\n",
        "        eta_max = base_lr * (decay_factor ** cycle)\n",
        "\n",
        "        # cosine schedule between (eta_min, max_lr]\n",
        "        lr = eta_min + 0.5 * (eta_max-eta_min) * (1 + math.cos(math.pi * t / T_current))\n",
        "        return lr/base_lr\n",
        "\n",
        "    return lr_lambda\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def perform_training(net,\n",
        "                     training_set,\n",
        "                     validation_set,\n",
        "                     epochs, w_decay, batch_size, sub_batch_size,\n",
        "                     lr, lr_lambda: cosannealing_decay_warmup,\n",
        "                     pretrained: bool | str = False):\n",
        "\n",
        "    assert batch_size % sub_batch_size == 0 #screws up gradient accumulation otherwise\n",
        "\n",
        "    printshare(\"training preparation...\")\n",
        "\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "    train_loader = DataLoader(training_set, batch_size=sub_batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_loader = DataLoader(validation_set, batch_size=sub_batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    #========= loading the checkpoint and preparing optimizers =========\n",
        "\n",
        "    criterion = nn.L1Loss()\n",
        "    optimizer = AdamW(\n",
        "        params=filter(lambda p: p.requires_grad, net.parameters()),\n",
        "        lr=lr, weight_decay=w_decay)\n",
        "        #[\n",
        "        #    {\"params\": net.features[-2].parameters()},  # last residual block\n",
        "        #    {\"params\": net.features[-1].parameters()},  # last conv\n",
        "        #    {\"params\": net.classifier.parameters()}  # classifier\n",
        "        #],\n",
        "\n",
        "    #used LambdaLR to implement CosineAnnealing with warm restarts and decay.\n",
        "    #yup, we need the base_lr to be passed in, cause it looks like this is the safest way.\n",
        "    scheduler = LambdaLR(\n",
        "        optimizer,\n",
        "        lr_lambda=lr_lambda\n",
        "    )\n",
        "\n",
        "    #scheduler = CosineAnnealingLR(\n",
        "    #    optimizer=optimizer,\n",
        "    #    T_max=50,\n",
        "    #    eta_min=1e-8,\n",
        "    #)\n",
        "\n",
        "    curr_epoch = 0\n",
        "    if isinstance(pretrained, str):\n",
        "        printshare(\"Loading pretrained model, optimizer & scheduler state dicts...\")\n",
        "        checkpoint = torch.load(pretrained)\n",
        "        mid_se_keys = [\"mid_se.fc.0.weight\", \"mid_se.fc.0.bias\", \"mid_se.fc.2.weight\", \"mid_se.fc.2.bias\"]\n",
        "\n",
        "        if 'model' not in checkpoint:\n",
        "            missing, unexpected = net.load_state_dict(checkpoint, strict=False)\n",
        "            printshare(\"got no optimizer & scheduler state dicts. model state dict set up successfully.\")\n",
        "\n",
        "        else:\n",
        "            missing, unexpected = net.load_state_dict(checkpoint['model'], strict=False)\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = w_decay\n",
        "\n",
        "            #scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
        "            scheduler.last_epoch = checkpoint['epoch']\n",
        "            curr_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "            printshare(\"all the dicts set up successfully.\")\n",
        "\n",
        "\n",
        "        printshare(f\"[DEBUG] model missing statedict vals: {missing};\")\n",
        "        printshare(f\"[DEBUG] model unexpected statedict vals: {unexpected}\")\n",
        "\n",
        "    #manual testing cycle\n",
        "    #while(True):\n",
        "\n",
        "    #    image, _ = training_set[225]\n",
        "    #    transform = v2.ToPILImage()\n",
        "    #    for i in range(16):\n",
        "    #        img = transform(image[i])\n",
        "    #        plt.imshow(img)\n",
        "    #        plt.title(f\"Augmented sample #0\")\n",
        "    #        plt.axis('off')\n",
        "    #        plt.show()\n",
        "\n",
        "    os.makedirs(relative_drive_path(\"checkpoints\"), exist_ok=True)\n",
        "    os.makedirs(relative_drive_path(\"checkpoints/stats\"), exist_ok=True)\n",
        "    printshare(\"done.\")\n",
        "\n",
        "    #========== training itself ==========\n",
        "    while curr_epoch < epochs:\n",
        "        printshare(f\"[{curr_time().strftime('%Y-%m-%d %H:%M:%S')}] epoch {curr_epoch + 1}/{epochs} processing...\")\n",
        "        output_train_msssim, input_train_msssim, train_loss = perform_training_epoch(\n",
        "            net=net,\n",
        "            full_batch_size=batch_size, sub_batch_size=sub_batch_size,\n",
        "            train_loader=train_loader,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            scaler=scaler\n",
        "        )\n",
        "\n",
        "        printshare(f\"training done. input ms-ssim: {round(100*input_train_msssim, 3)}%;\"+\n",
        "                   f\" output ms-ssim: {round(100*output_train_msssim, 3)}%\")\n",
        "\n",
        "\n",
        "        printshare(f\"[{curr_time().strftime('%Y-%m-%d %H:%M:%S')}] processing validation phase...\")\n",
        "        output_val_msssim, input_val_msssim, val_loss = perform_validation_epoch(\n",
        "            net=net,\n",
        "            val_loader=val_loader,\n",
        "            criterion=criterion\n",
        "        )\n",
        "\n",
        "        printshare(f\"validation done. input ms-ssim: {round(100*input_val_msssim, 3)}%;\" +\n",
        "                   f\" output ms-ssim: {round(100*output_val_msssim, 3)}%\")\n",
        "\n",
        "        torch.save({ # model\n",
        "            'model': net.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'scheduler': scheduler.state_dict(),\n",
        "            'epoch': curr_epoch,\n",
        "\n",
        "        }, relative_drive_path(f'checkpoints/ep_{curr_epoch+1}_ts_{round(100*output_train_msssim, 1)}_vs_{round(100*output_val_msssim, 1)}_model.pth'))\n",
        "\n",
        "        torch.save({ # stats\n",
        "            'epoch': curr_epoch,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'input_train_msssim': input_train_msssim,\n",
        "            'output_train_msssim': output_train_msssim,\n",
        "            'input_val_msssim': input_val_msssim,\n",
        "            'output_val_msssim': output_val_msssim\n",
        "        },\n",
        "            relative_drive_path(f'checkpoints/stats/ep_{curr_epoch+1}_ts_{round(100*output_train_msssim, 1)}_vs_{round(100*output_val_msssim, 1)}_stats.pth'))\n",
        "\n",
        "        curr_epoch += 1\n",
        "\n",
        "    printshare(f\"[{curr_time().strftime('%Y-%m-%d %H:%M:%S')}] training successfully finished.\")\n",
        "    return net\n",
        "\n",
        "\n",
        "def perform_training_epoch(net, full_batch_size, sub_batch_size,\n",
        "                           train_loader, criterion, optimizer, scheduler,\n",
        "                           scaler):\n",
        "    batch_losses = []\n",
        "    model_output_msssim_vals = []\n",
        "    input_msssim_vals = []\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    accum_steps = math.ceil(full_batch_size / sub_batch_size)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (input_imgs, target_imgs) in enumerate(train_loader):\n",
        "        input_imgs, target_imgs = input_imgs.cuda(), target_imgs.cuda()\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = net(input_imgs)\n",
        "            outputs = torch.clamp(outputs, 0.0, 1.0)\n",
        "\n",
        "            loss = criterion(outputs, target_imgs)\n",
        "            loss = loss / accum_steps\n",
        "\n",
        "        # MS-SSIM usually expects Float32\n",
        "        with torch.no_grad():\n",
        "            outputs_f32 = outputs.detach().float()\n",
        "\n",
        "            model_output_batch_msssim = ms_ssim(outputs_f32,\n",
        "                                                target_imgs,\n",
        "                                                data_range=1.0, size_average=True)\n",
        "            model_output_msssim_vals.append(model_output_batch_msssim.item())\n",
        "\n",
        "            input_batch_msssim = ms_ssim(input_imgs,\n",
        "                                         target_imgs,\n",
        "                                         data_range=1.0, size_average=True)\n",
        "            input_msssim_vals.append(input_batch_msssim.item())\n",
        "\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        batch_losses.append(loss.item() * accum_steps)\n",
        "\n",
        "        if (i + 1) % accum_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss = sum(batch_losses) / len(batch_losses)\n",
        "\n",
        "    avg_model_msssim = sum(model_output_msssim_vals) / len(model_output_msssim_vals) if len(\n",
        "        model_output_msssim_vals) > 0 else 0\n",
        "    avg_input_msssim = sum(input_msssim_vals) / len(input_msssim_vals) if len(input_msssim_vals) > 0 else 0\n",
        "\n",
        "    return avg_model_msssim, avg_input_msssim, epoch_loss\n",
        "\n",
        "\n",
        "def perform_validation_epoch(net, val_loader, criterion):\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        batch_losses = []\n",
        "        model_output_msssim_vals = []\n",
        "        input_msssim_vals = []\n",
        "\n",
        "        for input_imgs, target_imgs in val_loader:\n",
        "            input_imgs, target_imgs = input_imgs.cuda(), target_imgs.cuda()\n",
        "\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = net(input_imgs)\n",
        "                outputs = torch.clamp(outputs, 0.0, 1.0)\n",
        "\n",
        "                loss_val = criterion(outputs, target_imgs)\n",
        "\n",
        "            outputs_f32 = outputs.float()\n",
        "\n",
        "            model_batch_msssim = ms_ssim(outputs_f32,\n",
        "                                         target_imgs,\n",
        "                                         data_range=1.0, size_average=True)\n",
        "\n",
        "            input_batch_msssim = ms_ssim(input_imgs,\n",
        "                                         target_imgs,\n",
        "                                         data_range=1.0, size_average=True)\n",
        "\n",
        "            model_output_msssim_vals.append(model_batch_msssim.item())\n",
        "            input_msssim_vals.append(input_batch_msssim.item())\n",
        "            batch_losses.append(loss_val.item())\n",
        "\n",
        "        epoch_loss = sum(batch_losses) / len(batch_losses)\n",
        "\n",
        "        avg_model_msssim = sum(model_output_msssim_vals) / len(model_output_msssim_vals) if len(\n",
        "            model_output_msssim_vals) > 0 else 0\n",
        "        avg_input_msssim = sum(input_msssim_vals) / len(input_msssim_vals) if len(input_msssim_vals) > 0 else 0\n",
        "\n",
        "        return avg_model_msssim, avg_input_msssim, epoch_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def custom_loader(path):\n",
        "    return Image.open(path, formats=[\"JPEG\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    net = NAFNet()\n",
        "    net.cuda(0)\n",
        "\n",
        "    noise_transform = v2.Compose([\n",
        "        v2.ToImage(),\n",
        "        v2.Resize(size=(img_size, img_size)),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.GaussianNoise(mean=0, sigma=0.08),\n",
        "        BatchSaltAndPepper(salt_prob=0.05, pepper_prob=0.05),\n",
        "        v2.GaussianBlur(kernel_size=5, sigma=(0.5, 1.5))\n",
        "    ])\n",
        "\n",
        "    base_transform = v2.Compose([\n",
        "        v2.ToImage(),\n",
        "        v2.Resize(size=(img_size, img_size)),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "    ])\n",
        "\n",
        "    train_set = CocoDenoisingDataset(relative_data_path(\"train\"), input_img_transform=noise_transform,\n",
        "                                     target_img_transform=base_transform)\n",
        "    val_set = CocoDenoisingDataset(relative_data_path(\"val\"), input_img_transform=noise_transform,\n",
        "                                     target_img_transform=base_transform)\n",
        "    test_set = CocoDenoisingDataset(relative_data_path(\"test\"), input_img_transform=noise_transform,\n",
        "                                     target_img_transform=base_transform)\n",
        "\n",
        "    printshare(f\"len_train: {len(train_set)}; len_val: {len(val_set)}; len_test: {len(test_set)}.\")\n",
        "    #perform_training(net, train_set, val_set,\n",
        "    #                 epochs=600, w_decay=1e-3, batch_size=128, sub_batch_size=32,\n",
        "    #                 lr=1e-3, lr_lambda=cosannealing_decay_warmup(\n",
        "    #                   warmup_steps=0, T_0=10, T_mult=1.1, decay_factor=0.9, base_lr=1e-3, eta_min=1e-8),\n",
        "    #                 pretrained=relative_drive_path('checkpoints/ep_5_ts_0.9_vs_0.9_model.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_testing(net, test_set, bs=128, weights_file=\"\"):\n",
        "    test_loader = DataLoader(test_set, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
        "    if isinstance(weights_file, str):\n",
        "        printshare(\"Loading pretrained model, optimizer & scheduler state dicts...\")\n",
        "        checkpoint = torch.load(weights_file)\n",
        "\n",
        "        if 'model' not in checkpoint:\n",
        "            _, _ = net.load_state_dict(checkpoint, strict=False)\n",
        "            printshare(\"got no optimizer & scheduler state dicts. model state dict set up successfully.\")\n",
        "\n",
        "        else:\n",
        "            _, _ = net.load_state_dict(checkpoint['model'], strict=False)\n",
        "\n",
        "            printshare(\"all the dicts set up successfully.\")\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        model_output_msssim_vals = []\n",
        "        input_msssim_vals = []\n",
        "\n",
        "        for input_imgs, target_imgs in test_loader:\n",
        "            input_imgs, target_imgs = input_imgs.cuda(), target_imgs.cuda()\n",
        "\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = net(input_imgs)\n",
        "                outputs = torch.clamp(outputs, 0.0, 1.0)\n",
        "\n",
        "            outputs_f32 = outputs.float()\n",
        "\n",
        "            model_batch_msssim = ms_ssim(outputs_f32,\n",
        "                                         target_imgs,\n",
        "                                         data_range=1.0, size_average=True)\n",
        "\n",
        "            input_batch_msssim = ms_ssim(input_imgs,\n",
        "                                         target_imgs,\n",
        "                                         data_range=1.0, size_average=True)\n",
        "\n",
        "            model_output_msssim_vals.append(model_batch_msssim.item())\n",
        "            input_msssim_vals.append(input_batch_msssim.item())\n",
        "\n",
        "        avg_output_msssim = sum(model_output_msssim_vals) / len(model_output_msssim_vals) if len(\n",
        "            model_output_msssim_vals) > 0 else 0\n",
        "        avg_input_msssim = sum(input_msssim_vals) / len(input_msssim_vals) if len(input_msssim_vals) > 0 else 0\n",
        "\n",
        "        printshare(f\"testing done. input ms-ssim: {round(100*avg_input_msssim, 3)}%;\" +\n",
        "                   f\" output ms-ssim: {round(100*avg_output_msssim, 3)}%\")\n",
        "perform_testing(net, test_set, weights_file=relative_drive_path('checkpoints/ep_11_ts_95.1_vs_95.1_model.pth'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vl6CxIA0uIn",
        "outputId": "d79f7cda-7b2e-45ef-aee0-1002d1b762eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained model, optimizer & scheduler state dicts...\n",
            "all the dicts set up successfully.\n",
            "testing done. input ms-ssim: 78.256%; output ms-ssim: 95.072%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dYgjI5hrChFG"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}